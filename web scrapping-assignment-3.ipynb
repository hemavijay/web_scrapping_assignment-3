{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f66eba2",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "The product to be searched will be taken as input from user.\n",
    "For e.g. If user input is ‘guitar’. Then search for guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a656516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hema\\anaconda3\\lib\\site-packages (4.1.3)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hema\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hema\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b669c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\users\\hema\\anaconda3\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hema\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hema\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7482164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f99a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0412107",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b822e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad8c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding thte element for job search bar\n",
    "\n",
    "search_field =driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute']\")\n",
    "search_field.send_keys(\"guitar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb774dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on the search button\n",
    "search_button =driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f3cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar_brand=[]\n",
    "guitar_name=[]\n",
    "guitar_price=[]\n",
    "guitar_return=[]\n",
    "guitar_expected_delivery=[]\n",
    "guitar_availability=[]\n",
    "guitar_prod_url=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708c57f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_aps_sr_pg1_1?ie=UTF8&adId=A061107531TUGN5RHMV0N&url=%2FKadence-Frontier-Acoustic-Guitar-Strings%2Fdp%2FB01GDZ46AA%2Fref%3Dsr_1_1_sspa%3Fcrid%3D3S7P62WVJ6J0F%26keywords%3Dguitar%26qid%3D1653219389%26smid%3DAM04Z7MH4HSDD%26sprefix%3Dguitar%252Caps%252C437%26sr%3D8-1-spons%26psc%3D1%26smid%3DAM04Z7MH4HSDD&qualifier=1653219389&id=4294159762928314&widgetName=sp_atf',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_aps_sr_pg1_1?ie=UTF8&adId=A06072772I2LJ344EB41Y&url=%2FPhotron-Acoustic-Cutaway-PH38C-BK%2Fdp%2FB076T8V8NB%2Fref%3Dsr_1_2_sspa%3Fcrid%3D3S7P62WVJ6J0F%26keywords%3Dguitar%26qid%3D1653219389%26sprefix%3Dguitar%252Caps%252C437%26sr%3D8-2-spons%26psc%3D1&qualifier=1653219389&id=4294159762928314&widgetName=sp_atf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have to go to each page to extract the above information\n",
    "\n",
    "urls =[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\"):\n",
    "    guitar_prod_url.append(i.get_attribute('href'))\n",
    "guitar_prod_url[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7545c91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"ba1f6de9-d16d-4973-94bb-0d5f8d2dab38\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"c74b19ab-2a7b-47cf-8c96-e882acf3a247\")>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand=driver.find_elements_by_xpath(\"//span[@class='a-size-base a-color-base a-text-normal']\")\n",
    "brand[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a6d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The guitar has a fret crafted from ebony wood dense and gives a clear, crisp and bright sound. It is strong and best suitable for guitars.',\n",
       " 'This guitar has smooth keys for tuning and easy adjusting. It is the best choice for beginners.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for a in brand:\n",
    "    try:\n",
    "        brand1=a.text\n",
    "        guitar_brand.append(brand1)\n",
    "    except:\n",
    "        guitar_brand.append(\"-\")\n",
    "guitar_brand[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc5588a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"e08ac6d8-b1e4-48d0-b4eb-08e5c3b65c86\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"1e877c0b-6a49-43b5-8e66-7945dc9e8a74\")>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price']\")\n",
    "price[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e961f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹5,399', '₹2,490']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for b in price:\n",
    "    try:\n",
    "        price1=b.text\n",
    "        guitar_price.append(price1)\n",
    "    except:\n",
    "        guitar_price.append(\"-\")\n",
    "guitar_price[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5275055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"2440350d-20b4-4226-992c-7f83a538478e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"c26ab201235f9b698d4e764eeba3f011\", element=\"b45b502a-96f3-4ba7-af00-0e824c3c998b\")>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_del=driver.find_elements_by_xpath(\"//span[@class='a-color-secondary']\")\n",
    "exp_del[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2feaf18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sponsored', 'Sponsored']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for c in exp_del:\n",
    "    try:\n",
    "        del1=c.text\n",
    "        guitar_expected_delivery.append(del1)\n",
    "    except:\n",
    "        guitar_expected_delivery.append(\"-\")\n",
    "guitar_expected_delivery[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6579c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 64 14 64\n"
     ]
    }
   ],
   "source": [
    "print(len(guitar_brand), len(guitar_price),len(guitar_expected_delivery), len(guitar_prod_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "793ffd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11d89e",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ffc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_URL = \"https://www.google.co.in/search?q=fruits&hl=en&tbm=isch&sxsrf=ALiCzsbTFyFTTxtAmVbZfHywxaEOxbY-zQ%3A1653207883977&source=hp&biw=1506&bih=702&ei=S_OJYo3yOdLN-Qaq8raQBA&iflsig=AJiK0e8AAAAAYooBWyXwo15Xtb4UqTvUOvs5GtHum6qy&ved=0ahUKEwjNw6al1_L3AhXSZt4KHSq5DUIQ4dUDCAc&uact=5&oq=fruits&gs_lcp=CgNpbWcQAzILCAAQgAQQsQMQgwEyCAgAEIAEELEDMgsIABCABBCxAxCDATIICAAQgAQQsQMyCwgAEIAEELEDEIMBMggIABCABBCxAzILCAAQgAQQsQMQgwEyCwgAEIAEELEDEIMBMggIABCABBCxAzIICAAQgAQQsQM6BwgjEOoCECc6BAgjECc6BQgAEIAEUMYEWJAMYKMRaAFwAHgAgAFKiAGeA5IBATaYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABCg&sclient=img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9438ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(search_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=input(\"waiting for user to imput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b198cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrolling all the way up\n",
    "driver.execute_script(\"window.scrollTo(0,0);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7775d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "page_html=driver.page_source\n",
    "pageSoup=bs4.BeautifulSoup(page_html,'html.parser')\n",
    "fruits=pageSoup.findAll('div',{'class':\"isv-r PNCib MSM1fd BUooTd\"})\n",
    "fruits[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_bas=[]\n",
    "cars_bas=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_fruits=len(fruits)\n",
    "#print('found $s image fruits'%(len_fruits))\n",
    "for i in fruits:\n",
    "    fru1=i.text\n",
    "    fruits_bas.append(fru1)\n",
    "fruits_bas[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_URL1 = 'https://www.google.co.in/search?q=cars&hl=en&tbm=isch&sxsrf=ALiCzsbTFyFTTxtAmVbZfHywxaEOxbY-zQ%3A1653207883977&source=hp&biw=1506&bih=702&ei=S_OJYo3yOdLN-Qaq8raQBA&iflsig=AJiK0e8AAAAAYooBWyXwo15Xtb4UqTvUOvs5GtHum6qy&ved=0ahUKEwjNw6al1_L3AhXSZt4KHSq5DUIQ4dUDCAc&uact=5&oq=fruits&gs_lcp=CgNpbWcQAzILCAAQgAQQsQMQgwEyCAgAEIAEELEDMgsIABCABBCxAxCDATIICAAQgAQQsQMyCwgAEIAEELEDEIMBMggIABCABBCxAzILCAAQgAQQsQMQgwEyCwgAEIAEELEDEIMBMggIABCABBCxAzIICAAQgAQQsQM6BwgjEOoCECc6BAgjECc6BQgAEIAEUMYEWJAMYKMRaAFwAHgAgAFKiAGeA5IBATaYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABCg&sclient=img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(search_URL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e37a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars=pageSoup.findAll('div',{'class':\"isv-r PNCib MSM1fd BUooTd\"})\n",
    "cars[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in cars:\n",
    "    car1=j.text\n",
    "    cars_bas.append(car1)\n",
    "cars_bas[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a50951",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_URL2 = 'https://www.google.co.in/search?q=Machine Learning&hl=en&tbm=isch&sxsrf=ALiCzsbTFyFTTxtAmVbZfHywxaEOxbY-zQ%3A1653207883977&source=hp&biw=1506&bih=702&ei=S_OJYo3yOdLN-Qaq8raQBA&iflsig=AJiK0e8AAAAAYooBWyXwo15Xtb4UqTvUOvs5GtHum6qy&ved=0ahUKEwjNw6al1_L3AhXSZt4KHSq5DUIQ4dUDCAc&uact=5&oq=fruits&gs_lcp=CgNpbWcQAzILCAAQgAQQsQMQgwEyCAgAEIAEELEDMgsIABCABBCxAxCDATIICAAQgAQQsQMyCwgAEIAEELEDEIMBMggIABCABBCxAzILCAAQgAQQsQMQgwEyCwgAEIAEELEDEIMBMggIABCABBCxAzIICAAQgAQQsQM6BwgjEOoCECc6BAgjECc6BQgAEIAEUMYEWJAMYKMRaAFwAHgAgAFKiAGeA5IBATaYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABCg&sclient=img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(search_URL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f591a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML=pageSoup.findAll('div',{'class':\"isv-r PNCib MSM1fd BUooTd\"})\n",
    "ML[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_learn=[]\n",
    "for k in ML:\n",
    "    ML1=k.text\n",
    "    Machine_learn.append(ML1)\n",
    "Machine_learn[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cars_bas), len(fruits_bas), len(Machine_learn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df[\"list of fruits\"]= fruits_bas\n",
    "df['list of cars'] = cars_bas\n",
    "df['Machine learn'] = Machine_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a6f32",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be \n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57267801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5527227",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c454476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding thte element for job search bar\n",
    "\n",
    "search_field =driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "search_field.send_keys(\"OnePlus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on the search button\n",
    "search_button =driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_name=[]\n",
    "Smartph_name=[]\n",
    "Color=[]\n",
    "RAM =[]\n",
    "ROM=[]\n",
    "Primary_cam=[]\n",
    "Secondary_cam=[]\n",
    "Display_size=[]\n",
    "Battery=[]\n",
    "Price=[]\n",
    "Prod_URL=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =0\n",
    "end =2\n",
    "\n",
    "for page in range(start,end):\n",
    "    try:\n",
    "        name =driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "        name[0:2]\n",
    "    \n",
    "        for a in name:\n",
    "            name1=a.text\n",
    "            Brand_name.append(name1)\n",
    "        Brand_name[0:2]\n",
    "    except:\n",
    "        Brand_name.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        Ph_name =driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "        Ph_name[0:2]\n",
    "    \n",
    "        for b in Ph_name:\n",
    "            phna1=b.text\n",
    "            Smartph_name.append(phna1)\n",
    "        Smartph_name[0:2]\n",
    "    except:\n",
    "        Smartph_name.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        ram1 =driver.find_elements_by_xpath(\"//div[@class='fMghEO']\")\n",
    "        ram1[0:2]\n",
    "    \n",
    "        for c in ram1:\n",
    "            ram=c.text\n",
    "            RAM.append(ram)\n",
    "        RAM[0:2]\n",
    "    except:\n",
    "        RAM.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        prim_cam =driver.find_elements_by_xpath(\"//li[@class='rgWa7D']\")\n",
    "        prim_cam[0:2]\n",
    "    \n",
    "        for d in prim_cam:\n",
    "            cam1=d.text\n",
    "            Primary_cam.append(cam1)\n",
    "        Primary_cam[0:2]\n",
    "    except:\n",
    "        Primary_cam.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        display =driver.find_elements_by_xpath(\"//li[@class='rgWa7D']\")\n",
    "        display[0:2]\n",
    "    \n",
    "        for d in display:\n",
    "            dis1=d.text\n",
    "            Display_size.append(dis1)\n",
    "        Display_size[0:2]\n",
    "    except:\n",
    "        Display_size.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        bat=driver.find_elements_by_xpath(\"//li[@class='rgWa7D']\")\n",
    "        bat[0:2]\n",
    "    \n",
    "        for e in bat:\n",
    "            bat1=e.text\n",
    "            Battery.append(bat1)\n",
    "        Battery[0:2]\n",
    "    except:\n",
    "        Battery.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        pric =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "        pric[0:2]\n",
    "    \n",
    "        for f in pric:\n",
    "            pric1=f.text\n",
    "            Price.append(pric1)\n",
    "        Price[0:2]\n",
    "    except:\n",
    "        Price.append(\"-\")  \n",
    "        \n",
    "    try:\n",
    "        url =driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "        url[0:2]\n",
    "    \n",
    "        for g in url:\n",
    "            url1=g.text\n",
    "            Prod_url.append(url1)\n",
    "        Prod_URL[0:2]\n",
    "    except:\n",
    "        Prod_URL.append(\"-\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand_name), len(Smartph_name), len(RAM),len(Primary_cam), len(Display_size),len(Battery),len(Price), len(Prod_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone =pd.DataFrame({})\n",
    "phone['Name'] =Brand_name\n",
    "phone['Smart ph name'] =Smartph_name\n",
    "phone['RAM'] =RAM\n",
    "phone['primary camera'] =Primary_cam\n",
    "phone['Display Size'] =Display_size\n",
    "phone['Battery']= Battery\n",
    "phone['Price']= Price\n",
    "phone['Prod_URL']= Prod_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeddf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df4779",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551217ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e208b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "    \n",
    "# Define a dictionary containing  data \n",
    "data = {'City':['Bangalore', 'Mumbai', 'Chennai', 'Delhi']} \n",
    "    \n",
    "# Convert the dictionary into DataFrame \n",
    "df = pd.DataFrame(data) \n",
    "    \n",
    "# Observe the result \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.geocoders import Nominatim\n",
    "   \n",
    "# declare an empty list to store\n",
    "# latitude and longitude of values \n",
    "# of city column\n",
    "longitude = []\n",
    "latitude = []\n",
    "   \n",
    "# function to find the coordinate\n",
    "# of a given city \n",
    "def findGeocode(city):\n",
    "       \n",
    "    # try and catch is used to overcome\n",
    "    # the exception thrown by geolocator\n",
    "    # using geocodertimedout  \n",
    "    try:\n",
    "          \n",
    "        # Specify the user_agent as your\n",
    "        # app name it should not be none\n",
    "        geolocator = Nominatim(user_agent=\"your_app_name\")\n",
    "          \n",
    "        return geolocator.geocode(city)\n",
    "      \n",
    "    except GeocoderTimedOut:\n",
    "          \n",
    "        return findGeocode(city)    \n",
    "  \n",
    "# each value from city column\n",
    "# will be fetched and sent to\n",
    "# function find_geocode   \n",
    "for i in (df[\"City\"]):\n",
    "      \n",
    "    if findGeocode(i) != None:\n",
    "           \n",
    "        loc = findGeocode(i)\n",
    "          \n",
    "        # coordinates returned from \n",
    "        # function is stored into\n",
    "        # two separate list\n",
    "        latitude.append(loc.latitude)\n",
    "        longitude.append(loc.longitude)\n",
    "       \n",
    "    # if coordinate for a city not\n",
    "    # found, insert \"NaN\" indicating \n",
    "    # missing value \n",
    "    else:\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the output produced as dataframe.\n",
    "\n",
    "# now add this column to dataframe\n",
    "df[\"Longitude\"] = longitude\n",
    "df[\"Latitude\"] = latitude\n",
    "  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc016b",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "html_response = requests.get(start_url)\n",
    "html_response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data_urls = [start_url]\n",
    "\n",
    "for h3_tag in soup.find_all(name=\"h3\"):\n",
    "    more_data_urls.append(h3_tag.find(name='a').get('href'))\n",
    "\n",
    "more_data_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_list = []\n",
    "column_name = ['Sr. No.', 'Date (dd/mm/yyyy)', 'Startup Name', 'Industry/ Vertical', 'Sub-Vertical', 'City / Location', 'Investors’ Name', 'Investment Type', 'Amount (in USD)']\n",
    "more_data_urls = set(more_data_urls)\n",
    "\n",
    "urls_count = 1\n",
    "for url in more_data_urls:\n",
    "    html_response = requests.get(url)\n",
    "    html_response.status_code\n",
    "    soup = BeautifulSoup(html_response.content, 'html.parser')\n",
    "    \n",
    "    class_list = []\n",
    "    for element in soup.find_all(class_=True):\n",
    "        class_list.extend(element[\"class\"])\n",
    "    class_list = [cls for cls in class_list if 'tablepress-id-' in cls] \n",
    "    \n",
    "    if len(class_list) < 1:\n",
    "        skip_first_row = True\n",
    "        class_list.append(None)\n",
    "        for class_ in class_list:\n",
    "            tbl=soup.find(name='table') #, class_=class_)\n",
    "\n",
    "            n_rows = 0\n",
    "            for tr in tbl.find_all('tr'):\n",
    "                if skip_first_row == True:\n",
    "                    skip_first_row = False\n",
    "                    continue\n",
    "                new_row = {}\n",
    "                for col_id, td in enumerate(tr.find_all('td')):\n",
    "                    if col_id < len(column_name):\n",
    "                        new_row[column_name[col_id]] = td.text\n",
    "                if not new_row == {}:\n",
    "                    n_rows += 1\n",
    "                    new_row_list.append(new_row)\n",
    "            #print(\"class_list-old:\", class_, len(new_row_list), n_rows, url)\n",
    "    else:\n",
    "        for class_ in class_list:\n",
    "            tbl=soup.find(name='table', class_=class_)\n",
    "\n",
    "            n_rows = 0\n",
    "            for tr in tbl.find_all('tr'):\n",
    "                new_row = {}\n",
    "                for col_id, td in enumerate(tr.find_all('td')):\n",
    "                    if col_id < len(column_name):\n",
    "                        new_row[column_name[col_id]] = td.text\n",
    "                if not new_row == {}:\n",
    "                    n_rows += 1\n",
    "                    new_row_list.append(new_row)\n",
    "            #print(\"class_list-new :\", class_, len(new_row_list), n_rows, url)\n",
    "\n",
    "data = pd.DataFrame(new_row_list, columns=column_name)\n",
    "print(\"Data shape :\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef3d15",
   "metadata": {},
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5703b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdced78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.get('https://www.digit.in/')\n",
    "driver.get('https://www.digit.in/search/?keyword=gaming%20laptops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75115147",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/search?q=best%20laptops%20under%2080000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen(url)\n",
    "page_html = page.read()\n",
    "page.close()\n",
    "page_soup = BeautifulSoup(page_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095acef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = page_soup.findAll(\"div\", { \"class\": \"_2kHMtA\"})\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BeautifulSoup.prettify(containers[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Name of the produt:\n",
    "\n",
    "container = containers[4]\n",
    "prod_name = container.div.img[\"alt\"]\n",
    "print(prod_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Original Price of the product:\n",
    "original_price = container.findAll(\"div\", {\"class\": \"_3I9_wc _27UcVY\"})\n",
    "print(original_price[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Discount Percentage on the product:\n",
    "discount_percent = container.findAll(\"div\", {\"class\": \"_3Ay6Sb\"})\n",
    "print(discount_percent[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa18d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Discounted Price of the product:\n",
    "discounted_price = container.findAll(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"})\n",
    "print(discounted_price[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Product Ratings:\n",
    "prod_ratings = container.findAll(\"span\", {\"class\": \"_1lRcqv\"})\n",
    "print(prod_ratings[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdac826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Number of Product Reviews:\n",
    "reviews = container.findAll(\"span\", {\"class\" : \"_2_R_DZ\"})\n",
    "print(reviews[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for container in containers[:5]:\n",
    "    product_name = container.findAll(\"div\", {\"class\": \"_4rR01T\"})    \n",
    "    prod_name = product_name[0].text.strip()\n",
    "    \n",
    "    original_price = container.findAll(\"div\", {\"class\": \"_3I9_wc _27UcVY\"})\n",
    "    original = original_price[0].text.strip()\n",
    "    \n",
    "    discount_percent = container.findAll(\"div\", {\"class\": \"_3Ay6Sb\"})\n",
    "    percent = discount_percent[0].text.strip()\n",
    "    \n",
    "    discounted_price = container.findAll(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"})\n",
    "    discount = discounted_price[0].text.strip()\n",
    "\n",
    "    rating_container = container.findAll(\"span\", {\"class\": \"_1lRcqv\"})\n",
    "    prod_rating = rating_container[0].text.strip()\n",
    "    \n",
    "    reviews_container = container.findAll(\"span\", {\"class\": \"_2_R_DZ\"})\n",
    "    reviews_rating = reviews_container[0].text\n",
    "    \n",
    "    print(\"\\033[1mProduct Name: \\n\"+ '\\033[0m'+ str(prod_name), \"\\n\"),\n",
    "    print(\"\\033[1mOriginal Price: \\n\"+ '\\033[0m'+ str(original), \"\\n\"),\n",
    "    print(\"\\033[1mDiscount Percentage: \\n\"+ '\\033[0m'+ str(percent), \"\\n\"),\n",
    "    print(\"\\033[1mDiscounted Price: \\n\"+ '\\033[0m'+ str(discount), \"\\n\"),\n",
    "    print(\"\\033[1mRatings: \\n\"+ '\\033[0m'+ prod_rating, \"\\n\"),\n",
    "    print(\"\\033[1mNumber of Reviews: \\n\"+ '\\033[0m'+ reviews_rating, \"\\n\"),\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ed199",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a956fdc",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. \n",
    "Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d499ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.forbes.com/billionaires/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_rank=[]\n",
    "bill_name=[]\n",
    "bill_networth=[]\n",
    "bill_age=[]\n",
    "bill_citizenship=[]\n",
    "bill_source=[]\n",
    "bill_industry=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5325bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=driver.find_elements_by_xpath(\"//div[@class='rank']\")     \n",
    "rank[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19989321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in rank:\n",
    "    ran1=a.text\n",
    "    bill_rank.append(ran1)\n",
    "bill_rank[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bad05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=driver.find_elements_by_xpath(\"//div[@class='personName']\")     \n",
    "name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in name:\n",
    "    nam1=b.text\n",
    "    bill_name.append(nam1)\n",
    "bill_name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=driver.find_elements_by_xpath(\"//div[@class='netWorth']\")     \n",
    "net[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3945d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in net:\n",
    "    net1=c.text\n",
    "    bill_networth.append(net1)\n",
    "bill_networth[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "age=driver.find_elements_by_xpath(\"//div[@class='age']\")     \n",
    "age[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in age:\n",
    "    age1=d.text\n",
    "    bill_age.append(age1)\n",
    "bill_age[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country=driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship ']\")     \n",
    "country[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in country:\n",
    "    count1=e.text\n",
    "    bill_citizenship.append(count1)\n",
    "bill_citizenship[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=driver.find_elements_by_xpath(\"//div[@class='source-column']\")     \n",
    "source[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in source:\n",
    "    sour1=f.text\n",
    "    bill_source.append(sour1)\n",
    "bill_source[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "category=driver.find_elements_by_xpath(\"//div[@class='category']\")     \n",
    "category[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in category:\n",
    "    cat1=g.text\n",
    "    bill_industry.append(cat1)\n",
    "bill_industry[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a51184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bill_rank),len(bill_name),len(bill_networth),len(bill_age), len(bill_citizenship), len(bill_source),len(bill_industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame({})\n",
    "df['Billionaire Rank'] =bill_rank\n",
    "df['Billionaire Name']=bill_name\n",
    "df['NetWorth']=bill_networth\n",
    "df['Age']=bill_age\n",
    "#df['Citizenship']=bill_citizenship\n",
    "df['Source']=bill_source\n",
    "df['Industry'] =bill_industry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22498e",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open file to write data extracted from youtube\n",
    "# Creates a new .csv file that the data will be written to\n",
    "\n",
    "csv_file = open('output_scraping.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a574131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write header names\n",
    "writer.writerow(\n",
    "    ['url', 'link_title', 'channel', 'no_of_views', 'time_uploaded', 'comment', 'author', 'comment_posted', \n",
    "     'no_of_replies','upvotes','downvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open chrome \n",
    "locationOfWebdriver = \"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)\n",
    "youtube_pages = \"https://www.youtube.com/\"\n",
    "driver.get(youtube_pages)\n",
    "time.sleep(10)\n",
    "try:\n",
    "    print(\"=\" * 40)  # Shows in terminal when youtube summary page with search keyword is being scraped\n",
    "    print(\"Scraping \" + youtube_pages)\n",
    "    search = driver.find_element_by_id('search')\n",
    "    search.send_keys(\"Kishore Kumar\")    \n",
    "    driver.find_element_by_id('search-icon-legacy').click()\n",
    "    time.sleep(20)    \n",
    "    vtitle = driver.find_elements_by_id('video-title')\n",
    "    subscription = driver.find_elements_by_id('byline')\n",
    "    views = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[1]')\n",
    "    posted = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[2]')\n",
    "    \n",
    "    tcount = 0\n",
    "    href = []\n",
    "    title = []\n",
    "    channel = []\n",
    "    numview = []\n",
    "    postdate = []\n",
    "        \n",
    "    while tcount < 10:\n",
    "        href.append(vtitle[tcount].get_attribute('href'))\n",
    "        channel.append(subscription[tcount].get_attribute('title'))\n",
    "        title.append(vtitle[tcount].text)\n",
    "        numview.append(views[tcount].text)\n",
    "        postdate.append(posted[tcount].text)  \n",
    "        tcount = tcount +1\n",
    "    \n",
    "    # launch top ten extracted links and extract comment details\n",
    "    tcount = 0    \n",
    "    while tcount < 10:  \n",
    "        youtube_dict ={}\n",
    "        # extract comment section of top ten links\n",
    "        url = href[tcount]\n",
    "        print (url)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            print(\"+\" * 40)  # Shows in terminal when details of a new video is being scraped\n",
    "            print(\"Scraping child links \")\n",
    "            #scroll down to load comments\n",
    "            driver.execute_script('window.scrollTo(0,390);')\n",
    "            time.sleep(15)\n",
    "            #sort by top comments\n",
    "            sort= driver.find_element_by_xpath(\"\"\"//*[@id=\"icon-label\"]\"\"\")\n",
    "            sort.click()\n",
    "            time.sleep(10)\n",
    "            topcomments =driver.find_element_by_xpath(\"\"\"//*[@id=\"menu\"]/a[1]/paper-item/paper-item-body/div[1]\"\"\")\n",
    "            topcomments.click()\n",
    "            time.sleep(10)\n",
    "            # Loads 20 comments , scroll two times to load next set of 40 comments. \n",
    "            for i in range(0,2):\n",
    "                driver.execute_script(\"window.scrollTo(0,Math.max(document.documentElement.scrollHeight,document.body.scrollHeight,document.documentElement.clientHeight))\")\n",
    "                time.sleep(10)\n",
    "            \n",
    "            #count total number of comments and set index to number of comments if less than 50 otherwise set as 50. \n",
    "            totalcomments= len(driver.find_elements_by_xpath(\"\"\"//*[@id=\"content-text\"]\"\"\"))\n",
    "         \n",
    "            if totalcomments < 50:\n",
    "                index= totalcomments\n",
    "            else:\n",
    "                index= 50 \n",
    "                \n",
    "            ccount = 0\n",
    "            while ccount < index: \n",
    "                try:\n",
    "                    comment = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')[ccount].text\n",
    "                except:\n",
    "                    comment = \"\"\n",
    "                try:\n",
    "                    authors = driver.find_elements_by_xpath('//a[@id=\"author-text\"]/span')[ccount].text\n",
    "                except:\n",
    "                    authors = \"\"\n",
    "                try:\n",
    "                    comment_posted = driver.find_elements_by_xpath('//*[@id=\"published-time-text\"]/a')[ccount].text\n",
    "                except:\n",
    "                    comment_posted = \"\"\n",
    "                try:\n",
    "                    replies = driver.find_elements_by_xpath('//*[@id=\"more-text\"]')[ccount].text                    \n",
    "                    if replies ==\"View reply\":\n",
    "                        replies= 1\n",
    "                    else:\n",
    "                        replies =replies.replace(\"View \",\"\")\n",
    "                        replies =replies.replace(\" replies\",\"\")\n",
    "                except:\n",
    "                    replies = \"\"\n",
    "                try:\n",
    "                    upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')[ccount].text\n",
    "                except:\n",
    "                    upvotes = \"\"\n",
    "                        \n",
    "                youtube_dict['url'] = href[tcount]\n",
    "                youtube_dict['link_title'] = title[tcount]\n",
    "                youtube_dict['channel'] = channel[tcount]\n",
    "                youtube_dict['no_of_views'] = numview[tcount]\n",
    "                youtube_dict['time_uploaded'] =  postdate[tcount]\n",
    "                youtube_dict['comment'] = comment\n",
    "                youtube_dict['author'] = authors\n",
    "                youtube_dict['comment_posted'] = comment_posted\n",
    "                youtube_dict['no_of_replies'] = replies\n",
    "                youtube_dict['upvotes'] = upvotes\n",
    "                \n",
    "                writer.writerow(youtube_dict.values())\n",
    "                ccount = ccount +1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            driver.close()\n",
    "        tcount = tcount +1 \n",
    "    print(\"Scrapping process Completed\")   \n",
    "    csv_file.close()    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "   # driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433708d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "# importing the libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "  \n",
    "# creating function\n",
    "def scrape_info(url):\n",
    "      \n",
    "    # getting the request from url\n",
    "    r = requests.get(url)\n",
    "      \n",
    "    # converting the text\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "      \n",
    "    # finding meta info for title\n",
    "    title = s.find(\"span\", class_=\"watch-title\").text.replace(\"\\n\", \"\")\n",
    "      \n",
    "    # finding meta info for views\n",
    "    views = s.find(\"div\", class_=\"watch-view-count\").text\n",
    "      \n",
    "    # finding meta info for likes\n",
    "    likes = s.find(\"span\", class_=\"like-button-renderer\").span.button.text\n",
    "      \n",
    "    # saving this data in dictionary\n",
    "    data = {'title':title, 'views':views, 'likes':likes}\n",
    "      \n",
    "    # returning the dictionary\n",
    "    return data\n",
    "  \n",
    "# main function\n",
    "if __name__ == \"__main__\":\n",
    "      \n",
    "    # URL of the video\n",
    "    url =\"https://www.youtube.com/watch?time_continue=17&v=2wEA8nuThj8\"\n",
    "      \n",
    "    # calling the function\n",
    "    data = scrape_info(url)\n",
    "      \n",
    "    # printing the dictionary\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549f730",
   "metadata": {},
   "source": [
    "# 10 Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, \n",
    "overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hema\\\\OneDrive\\\\Documents\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.hostelworld.com/st/hostels/europe/england/london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db905175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to go to each page to extract the possible information\n",
    "\n",
    "hostel_url=[]\n",
    "urls =[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='property-card-container']\"):\n",
    "    hostel_url.append(i.get_attribute('href'))\n",
    "len( hostel_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostel_name =[]\n",
    "hostel_dist =[]\n",
    "hostel_rating =[]\n",
    "hostel_tot_review=[]\n",
    "hostel_over_review=[] \n",
    "hostel_private=[]\n",
    "hostel_dorms=[]\n",
    "hostel_facility=[]\n",
    "property_des=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113110f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =0\n",
    "end =2\n",
    "\n",
    "for page in range(start,end):\n",
    "    try:\n",
    "        name =driver.find_elements_by_xpath(\"//div[@class='property-name']\")\n",
    "        name[0:2]\n",
    "    \n",
    "        for a in name:\n",
    "            name1=a.text\n",
    "            hostel_name.append(name1)\n",
    "        hostel_name[0:2]\n",
    "    except:\n",
    "        hostel_name.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        distance =driver.find_elements_by_xpath(\"//span[@class='distance-description']\")\n",
    "        distance[0:2]\n",
    "    \n",
    "        for b in distance:\n",
    "            dist1=b.text\n",
    "            hostel_dist.append(dist1)\n",
    "        hostel_dist[0:2]\n",
    "    except:\n",
    "        hostel_dist.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        rating =driver.find_elements_by_xpath(\"//div[@class='score']\")\n",
    "        rating[0:2]\n",
    "    \n",
    "        for c in rating:\n",
    "            rat1=c.text\n",
    "            hostel_rating.append(rat1)\n",
    "        hostel_rating[0:2]\n",
    "    except:\n",
    "        hostel_rating.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        tot_review =driver.find_elements_by_xpath(\"//div[@class='keyword']\")\n",
    "        tot_review[0:2]\n",
    "    \n",
    "        for d in tot_review:\n",
    "            rev1=d.text\n",
    "            hostel_tot_review.append(rev1)\n",
    "        hostel_tot_review[0:2]\n",
    "    except:\n",
    "        hostel_tot_review.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        over_review =driver.find_elements_by_xpath(\"//div[@class='text']\")\n",
    "        over_review[0:2]\n",
    "    \n",
    "        for d in over_review:\n",
    "            rev1=d.text\n",
    "            hostel_over_review.append(rev1)\n",
    "        hostel_over_review[0:2]\n",
    "    except:\n",
    "        hostel_over_review.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        priv_from =driver.find_elements_by_xpath(\"//div[@class='accommodation-price single-row']\")\n",
    "        priv_from[0:2]\n",
    "    \n",
    "        for e in priv_from:\n",
    "            pri1=e.text\n",
    "            hostel_private.append(pri1)\n",
    "        hostel_private[0:2]\n",
    "    except:\n",
    "        hostel_private.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        dorms_from =driver.find_elements_by_xpath(\"//div[@class='accommodation-price single-row']\")\n",
    "        dorms_from[0:2]\n",
    "    \n",
    "        for f in dorms_from:\n",
    "            dorm=f.text\n",
    "            hostel_dorms.append(dorm)\n",
    "        hostel_dorms[0:2]\n",
    "    except:\n",
    "        hostel_dorms.append(\"-\")  \n",
    "        \n",
    "    try:\n",
    "        prop_des =driver.find_elements_by_xpath(\"//div[@class='accommodation-price single-row']\")\n",
    "        prop_des[0:2]\n",
    "    \n",
    "        for g in prop_des:\n",
    "            desp=g.text\n",
    "            property_des.append(dorm)\n",
    "        property_des[0:2]\n",
    "    except:\n",
    "        property_des.append(\"-\")  \n",
    "    \n",
    "    #nxt_url =driver.find_elements_by_xpath(\"//a[@class='property-card-container']\")\n",
    "    \n",
    "    #try:\n",
    "     #   driver.get(nxt_url[1].get_attribute('href'))\n",
    "    #except:\n",
    "     #   driver.get(nxt_url[0].get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostels =pd.DataFrame({})\n",
    "hostels['Name'] =hostel_name\n",
    "hostels['Distance'] =hostel_dist\n",
    "hostels['Ratings'] =hostel_rating\n",
    "hostels['Total Review'] =hostel_tot_review\n",
    "hostels['Overall Review'] =hostel_over_review\n",
    "hostels['Private from price']= hostel_private\n",
    "hostels['Dorms from price']= hostel_dorms\n",
    "hostels['Property Description']= property_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43decc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hostel_name),len(hostel_dist),len(hostel_rating),len(hostel_tot_review),len(hostel_over_review),len(hostel_private), len(hostel_dorms),len(property_des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59832309",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd42ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
